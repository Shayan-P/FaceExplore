{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c373273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c82cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_with_texts(images, texts, num_each_row=5):\n",
    "    num_images = len(images)\n",
    "    num_rows = (num_images + num_each_row-1) // num_each_row\n",
    "    num_cols = min(num_images, num_each_row) \n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows * 2, num_cols, figsize=(15, 6*num_rows))\n",
    "\n",
    "    for i, (image, text) in enumerate(zip(images, texts)):\n",
    "        row = (i // num_cols) * 2\n",
    "        col = i % num_cols\n",
    "\n",
    "        copy_image = image.copy()\n",
    "        copy_image.thumbnail((300, 300))\n",
    "        axes[row, col].imshow(copy_image)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "        axes[row + 1, col].text(0.5, 0.5, text, ha='center', va='center', wrap=True)\n",
    "        axes[row + 1, col].axis('off')\n",
    "\n",
    "    if num_images < num_rows * num_cols:\n",
    "        for i in range(num_images, num_rows * num_cols):\n",
    "            row = (i // num_cols) * 2\n",
    "            col = i % num_cols\n",
    "            fig.delaxes(axes[row, col])\n",
    "            fig.delaxes(axes[row + 1, col])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_images(images, num_each_row=5):\n",
    "    num_images = len(images)\n",
    "    num_rows = (num_images + num_each_row-1) // num_each_row\n",
    "    num_cols = min(num_images, num_each_row)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 3*num_rows))\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        \n",
    "        copy_image = image.copy()\n",
    "        copy_image.thumbnail((300, 300))\n",
    "        axes[row, col].imshow(copy_image)\n",
    "        axes[row, col].axis('off')\n",
    "\n",
    "    if num_images < num_rows * num_cols:\n",
    "        for i in range(num_images, num_rows * num_cols):\n",
    "            row = i // num_cols\n",
    "            col = i % num_cols\n",
    "            fig.delaxes(axes[row, col])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_image_items(image_items, num_each_row=5, func=None):\n",
    "    if func is None:\n",
    "        func = lambda item: item.get_image()\n",
    "    plot_images([func(item) for item in tqdm(image_items)], num_each_row=num_each_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248940fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from copy import deepcopy\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, select_largest=False, keep_all=True,\n",
    "    device=device\n",
    ")\n",
    "resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbb3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageItem:\n",
    "    \"\"\"represents an image in the collection\"\"\"\n",
    "    def __init__(self, filepath: str):\n",
    "        self.filepath = filepath\n",
    "        self.face_bounding_boxes = self.get_faces_images()\n",
    "        self.embedding = None\n",
    "        self.dict = dict()\n",
    "\n",
    "    def get_image(self) -> Image:\n",
    "        image =  Image.open(self.filepath)\n",
    "        return image\n",
    "        \n",
    "    def get_faces_images(self):\n",
    "        \"\"\"returns bounding box over faces\"\"\"\n",
    "        image = Image.open(self.filepath)\n",
    "        image = image.convert('RGB')\n",
    "        boxes, _ = mtcnn.detect(image)\n",
    "        if boxes is None:\n",
    "            return []\n",
    "        return boxes\n",
    "    \n",
    "def draw_bounding_box(img, bb):\n",
    "        img = deepcopy(img)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        x1, y1, x2, y2 = bb\n",
    "        draw.line([(x1, y1), (x2, y1)], fill='red', width=15)\n",
    "        draw.line([(x2, y1), (x2, y2)], fill='red', width=15)\n",
    "        draw.line([(x2, y2), (x1, y2)], fill='red', width=15)\n",
    "        draw.line([(x1, y2), (x1, y1)], fill='red', width=15)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a492942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_file_search(path):\n",
    "    for thing in os.listdir(path):\n",
    "        cur = os.path.join(path, thing)\n",
    "        if os.path.isfile(cur):\n",
    "            yield os.path.normpath(cur)\n",
    "        elif os.path.isdir(cur):\n",
    "            yield from recursive_file_search(cur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db2f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = \"../image-serve-path/dataset/\"\n",
    "sample_path = \"../image-serve-path/sample-images/\"\n",
    "\n",
    "all_image_paths = list(recursive_file_search(root_path))\n",
    "all_sample_image_paths = list(recursive_file_search(sample_path))\n",
    "\n",
    "all_images = [ImageItem(path) for path in tqdm(all_image_paths)]\n",
    "all_samples = [ImageItem(path) for path in tqdm(all_sample_image_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f272e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.ToTensor()    \n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),  # todo. remove this. maybe removing this improves it\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def get_image_embedding(image: Image):\n",
    "    \"\"\"returns embedding of image\"\"\"\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0) # todo use batch instead of this\n",
    "    image = image.to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = resnet(image).cpu().numpy().reshape(-1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60549f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_embedding(item):\n",
    "    item.embedding = [get_image_embedding(item.get_image().crop(bb)) for bb in item.face_bounding_boxes]\n",
    "    \n",
    "for item in tqdm(all_images):\n",
    "    update_embedding(item)\n",
    "    \n",
    "for item in tqdm(all_samples):\n",
    "    update_embedding(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c512bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_score(all_samples, item: ImageItem):\n",
    "    if len(item.embedding) == 0:\n",
    "        return 0\n",
    "    sample_embeddings = []\n",
    "    global_embeddings = []\n",
    "\n",
    "    global_embeddings.extend(item.embedding)\n",
    "    for item in all_samples:\n",
    "        sample_embeddings.extend(item.embedding)\n",
    "\n",
    "    global_embeddings = np.array(global_embeddings)\n",
    "    global_embeddings = global_embeddings / np.linalg.norm(global_embeddings, axis=-1)[:, None]\n",
    "    sample_embeddings = np.array(sample_embeddings)\n",
    "    sample_embeddings = sample_embeddings / np.linalg.norm(sample_embeddings, axis=-1)[:, None]\n",
    "\n",
    "    matrix = np.sum(global_embeddings[:, None, :] * sample_embeddings[None, :, :], axis=-1)\n",
    "    matrix_mean = np.mean(matrix, axis=1)\n",
    "    mx =  np.max(matrix_mean) # average over samples. maximum over globals\n",
    "    idx = np.argmax(matrix_mean)\n",
    "    return mx, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8826d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = [(item, *sim_score(all_samples, item)) for item in all_images] # item, max, max_id\n",
    "annot.sort(key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490e766",
   "metadata": {},
   "source": [
    "## load and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f974a48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save(filename, all_images, all_samples):    \n",
    "    with open(filename, 'wb') as f:\n",
    "            pickle.dump({\"all_images\": all_images, \"all_samples\": all_samples}, f)\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data[\"all_images\"], data[\"all_samples\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c420f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"checkpoint1\", all_images, all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63c0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_samples = load(\"checkpoint1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7345d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def small_image(img):\n",
    "    img.thumbnail((300, 300))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "images = [small_image(draw_bounding_box(p[0].get_image(), p[0].face_bounding_boxes[p[2]])) for p in tqdm(annot)]\n",
    "texts = [f'similarity: {p[1]}' for p in annot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4900855",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_with_bb = []\n",
    "\n",
    "for item in tqdm(all_images):\n",
    "    img = item.get_image()\n",
    "    for bb in item.face_bounding_boxes:\n",
    "        img = draw_bounding_box(img, bb)\n",
    "    img = small_image(img)\n",
    "    all_images_with_bb.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6f61b",
   "metadata": {},
   "source": [
    "# result: InceptionResnetV1 with Vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(all_images_with_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9640d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_with_texts(images, texts, num_each_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9e71f",
   "metadata": {},
   "source": [
    "## compare with face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875afe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "\n",
    "def get_image_embedding_fr(image: Image):\n",
    "    \"\"\"returns embedding of image\"\"\"\n",
    "    image.save(\"tmp.jpg\")\n",
    "    img = face_recognition.load_image_file(\"tmp.jpg\")\n",
    "    return face_recognition.face_encodings(img)\n",
    "\n",
    "\n",
    "def update_embedding(item):\n",
    "    item.embedding = get_image_embedding_fr(item.get_image())\n",
    "#     item.embedding = [get_image_embedding_fr(item.get_image().crop(bb)) for bb in item.face_bounding_boxes]\n",
    "\n",
    "\n",
    "for item in tqdm(all_images):\n",
    "    update_embedding(item)\n",
    "    \n",
    "for item in tqdm(all_samples):\n",
    "    update_embedding(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c7eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"checkpoint2\", all_images, all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_item(item):\n",
    "    image = item.get_image()\n",
    "    image.save(\"tmp.jpg\")\n",
    "    image = face_recognition.load_image_file(\"tmp.jpg\")\n",
    "\n",
    "    face_locations = face_recognition.face_locations(image)\n",
    "    face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "    __face_locations = []\n",
    "    for (a, b, c, d) in face_locations:\n",
    "        __face_locations.append((b, c, d, a))\n",
    "    item.face_bounding_boxes = __face_locations\n",
    "    item.embedding = face_encodings\n",
    "    \n",
    "for item in tqdm(all_images):\n",
    "    update_item(item)\n",
    "    \n",
    "for item in tqdm(all_samples):\n",
    "    update_item(item)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66376d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"checkpoint3\", all_images, all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52055f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = [(item, *sim_score(all_samples, item)) for item in all_images] # item, max, max_id\n",
    "annot.sort(key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33901e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [small_image(draw_bounding_box(p[0].get_image(), p[0].face_bounding_boxes[p[2]])) for p in tqdm(annot)]\n",
    "texts = [f'similarity: {p[1]}' for p in annot]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85228ec6",
   "metadata": {},
   "source": [
    "# result: Dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48277ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_with_texts(images, texts, num_each_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cf6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images_with_bb = []\n",
    "\n",
    "for item in tqdm(all_images):\n",
    "    img = item.get_image()\n",
    "    for bb in item.face_bounding_boxes:\n",
    "        img = draw_bounding_box(img, bb)\n",
    "    img = small_image(img)\n",
    "    all_images_with_bb.append(img)\n",
    "\n",
    "plot_images(all_images_with_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"checkpoint4\", all_images, all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_samples = load(\"checkpoint4\")\n",
    "\n",
    "# bug. coordinates of location were reversed\n",
    "def update_item(item):\n",
    "    bbs = []\n",
    "    for bb in item.face_bounding_boxes:\n",
    "        x1, y1, x2, y2 = bb\n",
    "        bbs.append((x2, y2, x1, y1))\n",
    "    item.face_bounding_boxes = bbs\n",
    "    \n",
    "for item in tqdm(all_images):\n",
    "    update_item(item)\n",
    "    \n",
    "for item in tqdm(all_samples):\n",
    "    update_item(item)    \n",
    "\n",
    "save(\"checkpoint5\", all_images, all_samples)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d3e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpall_images, cpall_samples = load(\"checkpoint5\")\n",
    "all_images, all_samples = load(\"checkpoint5\")\n",
    "\n",
    "def update_item(item, cpitem):\n",
    "    assert item.filepath == cpitem.filepath\n",
    "    image = item.get_image()\n",
    "    assert len(cpitem.embedding) == len(cpitem.face_bounding_boxes)\n",
    "    embeddings = []\n",
    "    for old_embedding, bb in zip(cpitem.embedding, cpitem.face_bounding_boxes):\n",
    "        nw_embedding = get_image_embedding(image.crop(bb))\n",
    "        embeddings.append(np.concatenate([3 * old_embedding, nw_embedding]))        \n",
    "    item.embedding = embeddings\n",
    "    \n",
    "for item, cpitem in tqdm(zip(all_images, cpall_images)):\n",
    "    update_item(item, cpitem)\n",
    "\n",
    "for item, cpitem in tqdm(zip(all_samples, cpall_samples)):\n",
    "    update_item(item, cpitem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c6b18",
   "metadata": {},
   "source": [
    "## result. concatenation of the two embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef88080",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = [(item, *sim_score(all_samples, item)) for item in all_images] # item, max, max_id\n",
    "annot.sort(key=lambda pair: pair[1], reverse=True)\n",
    "images = [small_image(draw_bounding_box(p[0].get_image(), p[0].face_bounding_boxes[p[2]])) for p in tqdm(annot)]\n",
    "texts = [f'similarity: {p[1]}' for p in annot]\n",
    "plot_images_with_texts(images, texts, num_each_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "save(\"checkpoint6\", all_images, all_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c76d5",
   "metadata": {},
   "source": [
    "## Clustering Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eefafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images, all_samples = load(\"checkpoint6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ff094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceItem:\n",
    "    def __init__(self, image_item, embedding, bb):\n",
    "        self.image_item = image_item\n",
    "        self.embedding = embedding\n",
    "        self.bb = bb\n",
    "\n",
    "    def get_cropped_image(self):\n",
    "        return self.image_item.get_image().crop(self.bb)\n",
    "\n",
    "all_faces = []\n",
    "\n",
    "for item in (all_samples + all_images):\n",
    "    for bb, emb in zip(item.face_bounding_boxes, item.embedding):\n",
    "        all_faces.append(FaceItem(item, emb, bb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef17bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "encodings = [face.embedding / np.linalg.norm(face.embedding) for face in all_faces]\n",
    "distances = pdist(encodings, metric='cosine')\n",
    "linkage_matrix = linkage(distances, method='complete')\n",
    "threshold = 0.1\n",
    "labels = fcluster(linkage_matrix, threshold, criterion='distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32ba66",
   "metadata": {},
   "source": [
    "# lol!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d53fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [small_image(face.image_item.get_image().crop(face.bb)) for i, face in enumerate(all_faces) if labels[i] == 151]\n",
    "plot_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade2dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(all_faces)))\n",
    "indices.sort(key=lambda i: labels[i])\n",
    "\n",
    "images = [small_image(all_faces[idx].image_item.get_image().crop(all_faces[idx].bb)) for idx in tqdm(indices)]\n",
    "texts = [f'label: {labels[idx]}' for idx in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59251d38",
   "metadata": {},
   "source": [
    "## Clustering works very well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba520a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_with_texts(images[:200], texts[:200], num_each_row=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c3e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, faces):\n",
    "        self.faces = faces\n",
    "\n",
    "\n",
    "all_clusters = []\n",
    "\n",
    "cluster_map = {}\n",
    "for face, label in zip(all_faces, labels):\n",
    "    mp = cluster_map.get(label, [])\n",
    "    cluster_map[label] = mp\n",
    "    mp.append(face)\n",
    "\n",
    "all_clusters = [Cluster(faces) for faces in cluster_map.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db5cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "\n",
    "for cluster in tqdm(all_clusters):\n",
    "    for i in range(3):\n",
    "        images.append(small_image(cluster.faces[min(i, len(cluster.faces)-1)].get_cropped_image()))\n",
    "\n",
    "plot_images(images, num_each_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09cfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_cluster(all_samples, cluster):\n",
    "    sample_embeddings = []\n",
    "    global_embeddings = []\n",
    "\n",
    "    for face in cluster.faces:\n",
    "        global_embeddings.append(face.embedding)\n",
    "    for item in all_samples:\n",
    "        sample_embeddings.extend(item.embedding)\n",
    "\n",
    "    # TODO DRY THIS OUT LATER\n",
    "    global_embeddings = np.array(global_embeddings)\n",
    "    global_embeddings = global_embeddings / np.linalg.norm(global_embeddings, axis=-1)[:, None]\n",
    "    sample_embeddings = np.array(sample_embeddings)\n",
    "    sample_embeddings = sample_embeddings / np.linalg.norm(sample_embeddings, axis=-1)[:, None]\n",
    "\n",
    "    matrix = np.sum(global_embeddings[:, None, :] * sample_embeddings[None, :, :], axis=-1)\n",
    "    matrix_mean = np.mean(matrix, axis=1)\n",
    "    mx =  np.max(matrix_mean) # average over samples. maximum over globals\n",
    "    idx = np.argmax(matrix_mean)\n",
    "    return mx, idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74756ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot = [(cluster, *sim_cluster(all_samples, cluster)) for cluster in all_clusters]\n",
    "annot.sort(key=lambda pair: pair[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5075fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "texts = []\n",
    "\n",
    "for cid, (cluster, score, idx) in enumerate(tqdm(annot)):\n",
    "    for face in cluster.faces:\n",
    "        img = face.image_item.get_image()\n",
    "        img = draw_bounding_box(img, face.bb)\n",
    "        images.append(small_image(img))\n",
    "        texts.append(f\"cluster: {cid}\\nscore: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aafd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_with_texts(images[:80], texts[:80], num_each_row=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
